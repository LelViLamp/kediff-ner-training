{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate transformers[sentencepiece] seqeval\n",
    "# !pip install accelerate\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, DataCollatorForTokenClassification, \\\n",
    "    AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint: str = \"dbmdz/bert-base-historic-multilingual-cased\"\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_aligned(\n",
    "        list1: list,\n",
    "        list2: list\n",
    "):\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for item1, item2 in zip(list1, list2):\n",
    "        max_length = max(len(item1), len(item2))\n",
    "        line1 += item1 + \" \" * (max_length - len(item1) + 1)\n",
    "        line2 += item2 + \" \" * (max_length - len(item2) + 1)\n",
    "    print(line1)\n",
    "    print(line2)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    print(\n",
    "        \"You work on Colab. Gentle as we are, we will mount Drive for you. \"\n",
    "        \"It'd help if you allowed this in the popup that opens.\"\n",
    "    )\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = os.path.join('drive', 'MyDrive', 'KEDiff', 'data')\n",
    "except:\n",
    "    print(\"You do not work on Colab\")\n",
    "    DATA_DIR = os.path.join('data')\n",
    "    pass\n",
    "\n",
    "print(f\"{DATA_DIR=}\", '-->', os.path.abspath(DATA_DIR))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILOUs_hug = Dataset.load_from_disk(dataset_path=os.path.join(DATA_DIR, 'BILOUs_hf'))\n",
    "print(\"Dataset:\", BILOUs_hug, sep='\\n')\n",
    "print(\"Features:\", BILOUs_hug.features, sep='\\n')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = BILOUs_hug.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "BILOUs_hug = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']}\n",
    ")\n",
    "del train_testvalid, test_valid\n",
    "print(BILOUs_hug)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser: BertTokenizerFast = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"Is '{model_checkpoint}' a fast tokeniser?\", tokeniser.is_fast)\n",
    "\n",
    "\n",
    "def batch_embed(batch):\n",
    "    # align annotation with added [CLS] and [SEP]\n",
    "    for column in [\n",
    "        'EVENT-BILOUs', 'LOC-BILOUs', 'MISC-BILOUs', 'ORG-BILOUs', 'PER-BILOUs', 'TIME-BILOUs',\n",
    "        'EVENT-IOBs', 'LOC-IOBs', 'MISC-IOBs', 'ORG-IOBs', 'PER-IOBs', 'TIME-IOBs'\n",
    "    ]:\n",
    "        all_labels = batch[column]\n",
    "        new_labels = [[-100, *labels[1:-1], -100] for labels in all_labels]\n",
    "        batch[column] = new_labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "BILOUs_hug = BILOUs_hug.map(batch_embed, batched=True)\n",
    "\n",
    "\n",
    "def batch_tokenise(batch):\n",
    "    # tokenise\n",
    "    tokenised_inputs = tokeniser(batch['Text'], truncation=True)\n",
    "    tokenised_inputs[\"labels\"] = batch['PER-IOBs']\n",
    "    return tokenised_inputs\n",
    "\n",
    "\n",
    "BILOUs_hug_tokenised = BILOUs_hug.map(\n",
    "    batch_tokenise,\n",
    "    batched=True,\n",
    "    remove_columns=BILOUs_hug[\"train\"].column_names\n",
    ")\n",
    "print(BILOUs_hug_tokenised)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = BILOUs_hug_tokenised[\"train\"][1]\n",
    "print(sample)\n",
    "del sample\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokeniser, padding=True)\n",
    "batch = data_collator([BILOUs_hug_tokenised[\"train\"][i] for i in range(2)])\n",
    "print(batch)\n",
    "print(batch['labels'])\n",
    "\n",
    "for i in range(2):\n",
    "    print(BILOUs_hug_tokenised[\"train\"][i][\"labels\"])\n",
    "del i\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = BILOUs_hug[\"train\"].features[\"PER-IOBs\"].feature.names\n",
    "\n",
    "batch = {'references': [], 'predictions': []}\n",
    "for i in [0, 1]:\n",
    "    labels = BILOUs_hug[\"train\"][i][\"PER-IOBs\"]\n",
    "    labels = [label_names[i] for i in labels[1:-1]]\n",
    "    # fake predictions\n",
    "    predictions = labels.copy()\n",
    "    predictions[2] = \"B-PER\"\n",
    "    predictions[3] = \"I-PER\"\n",
    "\n",
    "    print_aligned(labels, predictions)\n",
    "\n",
    "    batch['references'] += [labels]\n",
    "    batch['predictions'] += [predictions]\n",
    "del i, labels, predictions\n",
    "\n",
    "# calculate metrics\n",
    "for metric_name in [\"seqeval\", \"poseval\"]:\n",
    "    print(f\"Now evaluating using {metric_name=}\")\n",
    "    metric = evaluate.load(metric_name)\n",
    "    metric_result = metric.compute(predictions=batch['predictions'], references=batch['references'])\n",
    "    print(metric_result)\n",
    "# del batch, metric, metric_name, metric_result\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('poseval')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    metric_result = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"macro precision\": metric_result[\"macro avg\"][\"precision\"],\n",
    "        \"macro recall\": metric_result[\"macro avg\"][\"recall\"],\n",
    "        \"macro f1\": metric_result[\"macro avg\"][\"f1-score\"],\n",
    "        \"macro support\": metric_result[\"macro avg\"][\"support\"],\n",
    "\n",
    "        \"weighted precision\": metric_result[\"weighted avg\"][\"precision\"],\n",
    "        \"weighted recall\": metric_result[\"weighted avg\"][\"recall\"],\n",
    "        \"weighted f1\": metric_result[\"weighted avg\"][\"f1-score\"],\n",
    "        \"weighted support\": metric_result[\"weighted avg\"][\"support\"],\n",
    "\n",
    "        \"accuracy\": metric_result[\"accuracy\"],\n",
    "    }\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.config.num_labels\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_type in ['EVENT', 'LOC', 'MISC', 'ORG', 'PER', 'TIME']:\n",
    "    trained_model_name = f\"oalz-1788-q1-ner-{label_type}\"\n",
    "\n",
    "    print(f\"Now training '{trained_model_name}'\")\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir = os.path.join(DATA_DIR, trained_model_name),\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=BILOUs_hug_tokenised[\"train\"],\n",
    "        eval_dataset=BILOUs_hug_tokenised[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokeniser,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(DATA_DIR, trained_model_name))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}