{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, DataCollatorForTokenClassification, \\\n",
    "    AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint: str = \"dbmdz/bert-base-historic-multilingual-cased\"\n",
    "\n",
    "\n",
    "def print_aligned(\n",
    "        list1: list,\n",
    "        list2: list\n",
    "):\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for item1, item2 in zip(list1, list2):\n",
    "        max_length = max(len(item1), len(item2))\n",
    "        line1 += item1 + \" \" * (max_length - len(item1) + 1)\n",
    "        line2 += item2 + \" \" * (max_length - len(item2) + 1)\n",
    "    print(line1)\n",
    "    print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = os.path.join('drive', 'MyDrive', 'KEDiff', 'data')\n",
    "except:\n",
    "    DATA_DIR = os.path.join('data')\n",
    "    pass\n",
    "print(f\"{DATA_DIR=}\", '-->', os.path.abspath(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILOUs_hug = Dataset.load_from_disk(dataset_path=os.path.join(DATA_DIR, 'BILOUs_hf'))\n",
    "print(\"Dataset:\", BILOUs_hug, sep='\\n')\n",
    "print(\"Features:\", BILOUs_hug.features, sep='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = BILOUs_hug.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "BILOUs_hug = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']}\n",
    ")\n",
    "del train_testvalid, test_valid\n",
    "print(BILOUs_hug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser: BertTokenizerFast = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(f\"Is '{model_checkpoint}' a fast tokeniser?\", tokeniser.is_fast)\n",
    "\n",
    "\n",
    "def batch_embed(batch):\n",
    "    # align annotation with added [CLS] and [SEP]\n",
    "    for column in [\n",
    "        'EVENT-BILOUs', 'LOC-BILOUs', 'MISC-BILOUs', 'ORG-BILOUs', 'PER-BILOUs', 'TIME-BILOUs',\n",
    "        'EVENT-IOBs', 'LOC-IOBs', 'MISC-IOBs', 'ORG-IOBs', 'PER-IOBs', 'TIME-IOBs'\n",
    "    ]:\n",
    "        all_labels = batch[column]\n",
    "        new_labels = [[-100, *labels[1:-1], -100] for labels in all_labels]\n",
    "        batch[column] = new_labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "BILOUs_hug = BILOUs_hug.map(batch_embed, batched=True)\n",
    "\n",
    "\n",
    "def batch_tokenise(batch):\n",
    "    # tokenise\n",
    "    tokenised_inputs = tokeniser(batch['Text'], truncation=True)\n",
    "    tokenised_inputs[\"labels\"] = batch['PER-IOBs']\n",
    "    return tokenised_inputs\n",
    "\n",
    "\n",
    "BILOUs_hug_tokenised = BILOUs_hug.map(\n",
    "    batch_tokenise,\n",
    "    batched=True,\n",
    "    remove_columns=BILOUs_hug[\"train\"].column_names\n",
    ")\n",
    "print(BILOUs_hug_tokenised)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = BILOUs_hug_tokenised[\"train\"][1]\n",
    "sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokeniser, padding=True)\n",
    "batch = data_collator([BILOUs_hug_tokenised[\"train\"][i] for i in range(2)])\n",
    "print(batch)\n",
    "print(batch['labels'])\n",
    "\n",
    "for i in range(2):\n",
    "    print(BILOUs_hug_tokenised[\"train\"][i][\"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_names = BILOUs_hug[\"train\"].features[\"PER-IOBs\"].feature.names\n",
    "\n",
    "labels = BILOUs_hug[\"train\"][1][\"PER-IOBs\"]\n",
    "labels = [label_names[i] for i in labels[1:-1]]\n",
    "\n",
    "# fake predictions\n",
    "predictions = labels.copy()\n",
    "predictions[2] = \"B-PER\"\n",
    "predictions[3] = \"I-PER\"\n",
    "\n",
    "print_aligned(labels, predictions)\n",
    "metric.compute(predictions=[predictions], references=[labels])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.config.num_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = \"oalz-1788-q1-ner-PER\"\n",
    "args = TrainingArguments(\n",
    "    trained_model_name,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# todo store checkpoints on drive as well not just final model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=BILOUs_hug_tokenised[\"train\"],\n",
    "    eval_dataset=BILOUs_hug_tokenised[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokeniser,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(DATA_DIR, trained_model_name))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
